{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append('../../NeuralTuringMachine')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# from ntm import NTMCell\n",
    "\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-6472821aff36>:94: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-6472821aff36>:117: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step 1, Minibatch Loss= 2.2912, Training Accuracy= 0.188\n",
      "Step 10, Minibatch Loss= 2.2426, Training Accuracy= 0.312\n",
      "Step 20, Minibatch Loss= 2.0814, Training Accuracy= 0.602\n",
      "Step 30, Minibatch Loss= 1.9654, Training Accuracy= 0.578\n",
      "Step 40, Minibatch Loss= 1.7502, Training Accuracy= 0.594\n",
      "Step 50, Minibatch Loss= 1.6117, Training Accuracy= 0.680\n",
      "Step 60, Minibatch Loss= 1.4253, Training Accuracy= 0.688\n",
      "Step 70, Minibatch Loss= 1.2829, Training Accuracy= 0.688\n",
      "Step 80, Minibatch Loss= 1.1876, Training Accuracy= 0.711\n",
      "Step 90, Minibatch Loss= 1.1181, Training Accuracy= 0.711\n",
      "Step 100, Minibatch Loss= 1.0087, Training Accuracy= 0.688\n",
      "Step 110, Minibatch Loss= 1.0496, Training Accuracy= 0.719\n",
      "Step 120, Minibatch Loss= 0.9622, Training Accuracy= 0.703\n",
      "Step 130, Minibatch Loss= 1.0306, Training Accuracy= 0.648\n",
      "Step 140, Minibatch Loss= 0.8045, Training Accuracy= 0.758\n",
      "Step 150, Minibatch Loss= 0.7259, Training Accuracy= 0.781\n",
      "Step 160, Minibatch Loss= 0.7863, Training Accuracy= 0.773\n",
      "Step 170, Minibatch Loss= 1.0082, Training Accuracy= 0.688\n",
      "Step 180, Minibatch Loss= 0.9136, Training Accuracy= 0.695\n",
      "Step 190, Minibatch Loss= 0.7671, Training Accuracy= 0.750\n",
      "Step 200, Minibatch Loss= 0.8509, Training Accuracy= 0.719\n",
      "Step 210, Minibatch Loss= 0.9093, Training Accuracy= 0.711\n",
      "Step 220, Minibatch Loss= 0.8243, Training Accuracy= 0.742\n",
      "Step 230, Minibatch Loss= 0.8079, Training Accuracy= 0.719\n",
      "Step 240, Minibatch Loss= 0.7289, Training Accuracy= 0.758\n",
      "Step 250, Minibatch Loss= 0.7698, Training Accuracy= 0.734\n",
      "Step 260, Minibatch Loss= 0.7683, Training Accuracy= 0.750\n",
      "Step 270, Minibatch Loss= 0.8533, Training Accuracy= 0.734\n",
      "Step 280, Minibatch Loss= 0.7668, Training Accuracy= 0.727\n",
      "Step 290, Minibatch Loss= 0.6770, Training Accuracy= 0.781\n",
      "Step 300, Minibatch Loss= 0.8489, Training Accuracy= 0.680\n",
      "Step 310, Minibatch Loss= 0.7299, Training Accuracy= 0.750\n",
      "Step 320, Minibatch Loss= 0.6208, Training Accuracy= 0.789\n",
      "Step 330, Minibatch Loss= 0.7747, Training Accuracy= 0.742\n",
      "Step 340, Minibatch Loss= 0.7594, Training Accuracy= 0.750\n",
      "Step 350, Minibatch Loss= 0.6072, Training Accuracy= 0.789\n",
      "Step 360, Minibatch Loss= 0.7846, Training Accuracy= 0.711\n",
      "Step 370, Minibatch Loss= 0.6627, Training Accuracy= 0.781\n",
      "Step 380, Minibatch Loss= 0.7515, Training Accuracy= 0.734\n",
      "Step 390, Minibatch Loss= 0.7805, Training Accuracy= 0.750\n",
      "Step 400, Minibatch Loss= 0.7866, Training Accuracy= 0.750\n",
      "Step 410, Minibatch Loss= 0.8275, Training Accuracy= 0.695\n",
      "Step 420, Minibatch Loss= 0.6708, Training Accuracy= 0.758\n",
      "Step 430, Minibatch Loss= 0.7104, Training Accuracy= 0.742\n",
      "Step 440, Minibatch Loss= 0.6734, Training Accuracy= 0.781\n",
      "Step 450, Minibatch Loss= 0.7454, Training Accuracy= 0.719\n",
      "Step 460, Minibatch Loss= 0.7003, Training Accuracy= 0.734\n",
      "Step 470, Minibatch Loss= 0.8839, Training Accuracy= 0.664\n",
      "Step 480, Minibatch Loss= 0.6583, Training Accuracy= 0.742\n",
      "Step 490, Minibatch Loss= 0.7526, Training Accuracy= 0.703\n",
      "Step 500, Minibatch Loss= 0.7188, Training Accuracy= 0.766\n",
      "Step 510, Minibatch Loss= 0.6591, Training Accuracy= 0.758\n",
      "Step 520, Minibatch Loss= 0.6826, Training Accuracy= 0.742\n",
      "Step 530, Minibatch Loss= 0.7620, Training Accuracy= 0.703\n",
      "Step 540, Minibatch Loss= 0.5730, Training Accuracy= 0.789\n",
      "Step 550, Minibatch Loss= 0.7818, Training Accuracy= 0.719\n",
      "Step 560, Minibatch Loss= 0.5758, Training Accuracy= 0.773\n",
      "Step 570, Minibatch Loss= 0.6359, Training Accuracy= 0.766\n",
      "Step 580, Minibatch Loss= 0.6610, Training Accuracy= 0.766\n",
      "Step 590, Minibatch Loss= 0.7355, Training Accuracy= 0.727\n",
      "Step 600, Minibatch Loss= 0.7133, Training Accuracy= 0.719\n",
      "Step 610, Minibatch Loss= 0.6102, Training Accuracy= 0.773\n",
      "Step 620, Minibatch Loss= 0.7802, Training Accuracy= 0.734\n",
      "Step 630, Minibatch Loss= 0.5826, Training Accuracy= 0.812\n",
      "Step 640, Minibatch Loss= 0.8313, Training Accuracy= 0.688\n",
      "Step 650, Minibatch Loss= 0.7000, Training Accuracy= 0.750\n",
      "Step 660, Minibatch Loss= 0.6932, Training Accuracy= 0.742\n",
      "Step 670, Minibatch Loss= 0.7970, Training Accuracy= 0.719\n",
      "Step 680, Minibatch Loss= 0.6906, Training Accuracy= 0.750\n",
      "Step 690, Minibatch Loss= 0.5758, Training Accuracy= 0.773\n",
      "Step 700, Minibatch Loss= 0.7489, Training Accuracy= 0.695\n",
      "Step 710, Minibatch Loss= 0.7692, Training Accuracy= 0.688\n",
      "Step 720, Minibatch Loss= 0.7079, Training Accuracy= 0.750\n",
      "Step 730, Minibatch Loss= 0.5788, Training Accuracy= 0.797\n",
      "Step 740, Minibatch Loss= 0.7789, Training Accuracy= 0.688\n",
      "Step 750, Minibatch Loss= 0.7576, Training Accuracy= 0.719\n",
      "Step 760, Minibatch Loss= 0.5712, Training Accuracy= 0.789\n",
      "Step 770, Minibatch Loss= 0.6940, Training Accuracy= 0.727\n",
      "Step 780, Minibatch Loss= 0.6337, Training Accuracy= 0.766\n",
      "Step 790, Minibatch Loss= 0.5001, Training Accuracy= 0.836\n",
      "Step 800, Minibatch Loss= 0.7080, Training Accuracy= 0.742\n",
      "Step 810, Minibatch Loss= 0.8605, Training Accuracy= 0.688\n",
      "Step 820, Minibatch Loss= 0.4764, Training Accuracy= 0.812\n",
      "Step 830, Minibatch Loss= 0.7594, Training Accuracy= 0.711\n",
      "Step 840, Minibatch Loss= 0.6398, Training Accuracy= 0.758\n",
      "Step 850, Minibatch Loss= 0.6850, Training Accuracy= 0.742\n",
      "Step 860, Minibatch Loss= 0.6557, Training Accuracy= 0.758\n",
      "Step 870, Minibatch Loss= 0.6615, Training Accuracy= 0.758\n",
      "Step 880, Minibatch Loss= 0.6787, Training Accuracy= 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 890, Minibatch Loss= 0.4862, Training Accuracy= 0.828\n",
      "Step 900, Minibatch Loss= 0.5964, Training Accuracy= 0.781\n",
      "Step 910, Minibatch Loss= 0.6815, Training Accuracy= 0.758\n",
      "Step 920, Minibatch Loss= 0.7624, Training Accuracy= 0.711\n",
      "Step 930, Minibatch Loss= 0.7482, Training Accuracy= 0.719\n",
      "Step 940, Minibatch Loss= 0.4867, Training Accuracy= 0.820\n",
      "Step 950, Minibatch Loss= 0.5586, Training Accuracy= 0.781\n",
      "Step 960, Minibatch Loss= 0.4261, Training Accuracy= 0.867\n",
      "Step 970, Minibatch Loss= 0.5720, Training Accuracy= 0.789\n",
      "Step 980, Minibatch Loss= 0.5277, Training Accuracy= 0.812\n",
      "Step 990, Minibatch Loss= 0.7370, Training Accuracy= 0.703\n",
      "Step 1000, Minibatch Loss= 0.6544, Training Accuracy= 0.781\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.78125\n"
     ]
    }
   ],
   "source": [
    "def conv_to_fc(input_tensor):\n",
    "    n_hidden = tf.reduce_prod([v.value for v in input_tensor.get_shape()[1:]])\n",
    "    input_tensor = tf.reshape(input_tensor, [-1, n_hidden])\n",
    "    return input_tensor\n",
    "\n",
    "def linear(input_tensor, num_hidden, scope):\n",
    "    n_input = input_tensor.get_shape()[1].value\n",
    "    with tf.variable_scope(scope):\n",
    "        weight = tf.get_variable(\"w\", [n_input, num_hidden], initializer=tf.initializers.orthogonal())\n",
    "        bias = tf.get_variable(\"b\", [num_hidden], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input_tensor, weight) + bias\n",
    "    \n",
    "def linear2d(input_tensor, num_hidden, scope):\n",
    "    b, h, w = input_tensor.shape\n",
    "    with tf.variable_scope(scope):\n",
    "        tensors = []\n",
    "        for i in range(h):\n",
    "            weight = tf.get_variable(\"w\"+str(i), [w, num_hidden], initializer=tf.initializers.orthogonal())\n",
    "            bias = tf.get_variable(\"b\"+str(i), [num_hidden], initializer=tf.constant_initializer(0.0))\n",
    "            tensors.append(tf.matmul(input_tensor[:,i,:], weight) + bias)\n",
    "        return tf.stack(tensors, axis=1)\n",
    "    \n",
    "    \n",
    "def conv2d(input_tensor, num_filters, filter_size, stride, scope,  **kwargs):\n",
    "    num_channels = input_tensor.get_shape()[-1].value\n",
    "    filter_height = filter_width = filter_size\n",
    "    wshape = [filter_height, filter_width, num_channels, num_filters]\n",
    "    bshape = [1, 1, 1, num_filters]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    with tf.variable_scope(scope):\n",
    "        weights = tf.get_variable(\"w\", wshape, initializer=tf.initializers.orthogonal())\n",
    "        bias = tf.get_variable(\"b\", bshape, initializer=tf.constant_initializer(0.0))\n",
    "        return bias + tf.nn.conv2d(input_tensor, weights, strides=strides, padding='VALID', data_format='NHWC',  **kwargs)\n",
    "\n",
    "def softmax_2d(tensor):\n",
    "    b, h, w, c = tensor.shape\n",
    "    tensor = tf.reshape(tensor, (-1, h * w, c))\n",
    "    tensor = tf.nn.softmax(tensor, axis=1)\n",
    "    tensor = tf.reshape(tensor, (-1, h, w, c))\n",
    "    return tensor\n",
    "    \n",
    "def attention_block(tensor, g, scope):\n",
    "    b, h, w, f = tensor.shape\n",
    "    ls = tf.reshape(tensor, (-1, h*w, f))\n",
    "    print(\"ls\",ls.get_shape())\n",
    "    g_size = g.get_shape()[-1].value\n",
    "    print(\"g\", g.get_shape())\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        lsat = linear2d(ls, num_hidden=g_size, scope='lsat') # (-1, h*w, g_size)\n",
    "        lsat = tf.nn.relu(lsat)\n",
    "        print(\"lsat\", lsat.get_shape())\n",
    "        ### TODO is including also the batch dimension correct? ###\n",
    "        g_tiled = tf.tile(tf.reshape(g, (-1, 1, g_size)), [1, h*w, 1])\n",
    "        compatibility = tf.reduce_sum(tf.multiply(lsat, g_tiled), axis=-1, keepdims=True) #tf.tensordot(lsat, g_tiled, axes=((-1), (-1))) # (-1, h*w, 1)\n",
    "#         compatibility = tf.reshape(compatibility, shape=[-1, h*w, 1]) # (-1, h*w)\n",
    "        print(\"compatibility\", compatibility.get_shape())\n",
    "        attention = tf.nn.softmax(compatibility, axis=1) # (-1, h*w)\n",
    "    #     attention = tf.tile(tf.reshape(attention, shape=(-1, h*w, 1)), [1, 1, f]) # (-1, h*w, f)\n",
    "        attention_tiled = tf.tile(attention, [1, 1, f]) # (-1, h*w, f)\n",
    "        print(\"attention\", attention_tiled.get_shape())\n",
    "        weighted_ls = attention_tiled * ls\n",
    "        return weighted_ls, attention\n",
    "\n",
    "\n",
    "def cnn(X, num_classes, **kwargs):\n",
    "    batch_size = X.shape[0].value\n",
    "    scaled_images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = conv2d(input_tensor=scaled_images, num_filters=32, filter_size=2, stride=2, scope=\"conv1\")\n",
    "    conv1 = tf.nn.elu(conv1)\n",
    "#     conv1 = tf.nn.l2_normalize(conv1, axis=-1)\n",
    "    conv2 = conv2d(input_tensor=conv1, num_filters=64, filter_size=2, stride=2, scope=\"conv2\")\n",
    "    conv2 = tf.nn.elu(conv2)\n",
    "#     conv2 = tf.nn.l2_normalize(conv2, axis=-1)\n",
    "    \n",
    "    g = tf.nn.relu(linear(conv_to_fc(conv2), num_hidden=256, scope=\"ln1\"))\n",
    "    \n",
    "\n",
    "#     lst = memory_module(g)\n",
    "\n",
    "    last = g\n",
    "    \n",
    "    ln1 = linear(last, num_hidden=num_classes, scope=\"lnlast\")\n",
    "    \n",
    "    return tf.nn.relu(ln1, name=\"lastrelu\")\n",
    "\n",
    "\n",
    "if 'sess' in locals():\n",
    "    sess.close()\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "\n",
    "logits = cnn(X, num_classes)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1, num_steps+1):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                             Y: batch_y,\n",
    "                                                             keep_prob: 1.0})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for 256 MNIST test images\n",
    "print(\"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                  Y: mnist.test.labels[:256],\n",
    "                                  keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 9 9 4 1 8 2 1 2 9 7 0 9 2 6 4 1 8 8 2 9 2 0 4 0 0 2 8] ==\n",
      " [3 0 1 9 9 4 1 8 2 1 2 9 7 5 9 2 6 4 1 5 8 2 9 2 0 4 0 0 2 8] ?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_img = mnist.test.images[270:300]\n",
    "test_y = mnist.test.labels[270:300]\n",
    "\n",
    "res_op = tf.argmax(prediction, 1)\n",
    "\n",
    "    \n",
    "result = sess.run(res_op, feed_dict={X: np.reshape(test_img, (test_img.shape[0], -1))})\n",
    "print(result, \"==\\n\", np.argmax(test_y, 1), \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lastrelu:0\", shape=(?, 10), dtype=float32)\n",
      "[[-0.        -0.        -0.        -0.         7.1109514 -0.\n",
      "   1.3631005  0.9440105  1.4569883  2.459519 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### net visualization\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "extent = 0, 28, 0, 28\n",
    "\n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(1, figsize=(10,10))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\")\n",
    "        \n",
    "def plotAttention(units):\n",
    "    b, px, rest = units.shape\n",
    "    print(np.amax(units), np.amin(units))\n",
    "#     plt.figure(1, figsize=(20,20))\n",
    "    h = w = int(math.sqrt(px))\n",
    "    print(h, w)\n",
    "    units = np.reshape(units, (h, w))\n",
    "    plt.imshow(units, interpolation=\"nearest\", cmap=\"viridis\", alpha=0.5, extent=extent)\n",
    "\n",
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={X:np.reshape(stimuli,[1,784],order='F'),keep_prob:1.0})\n",
    "    plotAttention(units)\n",
    "\n",
    "    \n",
    "fig = plt.figure(frameon=True)\n",
    "\n",
    "imageToUse = mnist.test.images[np.random.randint(low=0, high=len(mnist.test.images))]\n",
    "plt.imshow(np.reshape(imageToUse,[28,28]), interpolation=\"nearest\", cmap=\"gray\", extent=extent)\n",
    "\n",
    "layer = tf.get_default_graph().get_tensor_by_name('lastrelu:0') \n",
    "print(layer)\n",
    "\n",
    "\n",
    "units = sess.run(layer,feed_dict={X:np.reshape(imageToUse,[1,784],order='F'),keep_prob:1.0})\n",
    "print(units)\n",
    "plt.imshow(units)\n",
    "\n",
    "# getActivations(conv1w, imageToUse)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUTO_REUSE',\n",
       " 'AggregationMethod',\n",
       " 'Assert',\n",
       " 'AttrValue',\n",
       " 'COMPILER_VERSION',\n",
       " 'CXX11_ABI_FLAG',\n",
       " 'ConditionalAccumulator',\n",
       " 'ConditionalAccumulatorBase',\n",
       " 'ConfigProto',\n",
       " 'DType',\n",
       " 'DeviceSpec',\n",
       " 'Dimension',\n",
       " 'Event',\n",
       " 'FIFOQueue',\n",
       " 'FixedLenFeature',\n",
       " 'FixedLenSequenceFeature',\n",
       " 'FixedLengthRecordReader',\n",
       " 'GIT_VERSION',\n",
       " 'GPUOptions',\n",
       " 'GRAPH_DEF_VERSION',\n",
       " 'GRAPH_DEF_VERSION_MIN_CONSUMER',\n",
       " 'GRAPH_DEF_VERSION_MIN_PRODUCER',\n",
       " 'GradientTape',\n",
       " 'Graph',\n",
       " 'GraphDef',\n",
       " 'GraphKeys',\n",
       " 'GraphOptions',\n",
       " 'HistogramProto',\n",
       " 'IdentityReader',\n",
       " 'IndexedSlices',\n",
       " 'InteractiveSession',\n",
       " 'LMDBReader',\n",
       " 'LogMessage',\n",
       " 'MONOLITHIC_BUILD',\n",
       " 'MetaGraphDef',\n",
       " 'NameAttrList',\n",
       " 'NoGradient',\n",
       " 'NodeDef',\n",
       " 'NotDifferentiable',\n",
       " 'OpError',\n",
       " 'Operation',\n",
       " 'OptimizerOptions',\n",
       " 'PaddingFIFOQueue',\n",
       " 'Print',\n",
       " 'PriorityQueue',\n",
       " 'QUANTIZED_DTYPES',\n",
       " 'QueueBase',\n",
       " 'RaggedTensor',\n",
       " 'RandomShuffleQueue',\n",
       " 'ReaderBase',\n",
       " 'RegisterGradient',\n",
       " 'RunMetadata',\n",
       " 'RunOptions',\n",
       " 'Session',\n",
       " 'SessionLog',\n",
       " 'SparseConditionalAccumulator',\n",
       " 'SparseFeature',\n",
       " 'SparseTensor',\n",
       " 'SparseTensorValue',\n",
       " 'Summary',\n",
       " 'SummaryMetadata',\n",
       " 'TFRecordReader',\n",
       " 'Tensor',\n",
       " 'TensorArray',\n",
       " 'TensorInfo',\n",
       " 'TensorShape',\n",
       " 'TensorSpec',\n",
       " 'TextLineReader',\n",
       " 'UnconnectedGradients',\n",
       " 'VERSION',\n",
       " 'VarLenFeature',\n",
       " 'Variable',\n",
       " 'VariableAggregation',\n",
       " 'VariableScope',\n",
       " 'VariableSynchronization',\n",
       " 'WholeFileReader',\n",
       " '_CONTRIB_WARNING',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__compiler_version__',\n",
       " '__cxx11_abi_flag__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__git_version__',\n",
       " '__loader__',\n",
       " '__monolithic_build__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_absolute_import',\n",
       " '_api',\n",
       " '_component_api_helper',\n",
       " '_division',\n",
       " '_names_with_underscore',\n",
       " '_os',\n",
       " '_print_function',\n",
       " '_tf_api_dir',\n",
       " 'abs',\n",
       " 'accumulate_n',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add',\n",
       " 'add_check_numerics_ops',\n",
       " 'add_n',\n",
       " 'add_to_collection',\n",
       " 'add_to_collections',\n",
       " 'all_variables',\n",
       " 'angle',\n",
       " 'app',\n",
       " 'arg_max',\n",
       " 'arg_min',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_dtype',\n",
       " 'as_string',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_equal',\n",
       " 'assert_greater',\n",
       " 'assert_greater_equal',\n",
       " 'assert_integer',\n",
       " 'assert_less',\n",
       " 'assert_less_equal',\n",
       " 'assert_near',\n",
       " 'assert_negative',\n",
       " 'assert_non_negative',\n",
       " 'assert_non_positive',\n",
       " 'assert_none_equal',\n",
       " 'assert_positive',\n",
       " 'assert_proper_iterable',\n",
       " 'assert_rank',\n",
       " 'assert_rank_at_least',\n",
       " 'assert_rank_in',\n",
       " 'assert_same_float_dtype',\n",
       " 'assert_scalar',\n",
       " 'assert_type',\n",
       " 'assert_variables_initialized',\n",
       " 'assign',\n",
       " 'assign_add',\n",
       " 'assign_sub',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'autograph',\n",
       " 'batch_gather',\n",
       " 'batch_scatter_update',\n",
       " 'batch_to_space',\n",
       " 'batch_to_space_nd',\n",
       " 'betainc',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitcast',\n",
       " 'bitwise',\n",
       " 'bool',\n",
       " 'boolean_mask',\n",
       " 'broadcast_dynamic_shape',\n",
       " 'broadcast_static_shape',\n",
       " 'broadcast_to',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'ceil',\n",
       " 'check_numerics',\n",
       " 'cholesky',\n",
       " 'cholesky_solve',\n",
       " 'clip_by_average_norm',\n",
       " 'clip_by_global_norm',\n",
       " 'clip_by_norm',\n",
       " 'clip_by_value',\n",
       " 'colocate_with',\n",
       " 'compat',\n",
       " 'compiler',\n",
       " 'complex',\n",
       " 'complex128',\n",
       " 'complex64',\n",
       " 'concat',\n",
       " 'cond',\n",
       " 'confusion_matrix',\n",
       " 'conj',\n",
       " 'constant',\n",
       " 'constant_initializer',\n",
       " 'container',\n",
       " 'contrib',\n",
       " 'control_dependencies',\n",
       " 'convert_to_tensor',\n",
       " 'convert_to_tensor_or_indexed_slices',\n",
       " 'convert_to_tensor_or_sparse_tensor',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count_nonzero',\n",
       " 'count_up_to',\n",
       " 'create_partitioned_variables',\n",
       " 'cross',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'custom_gradient',\n",
       " 'data',\n",
       " 'debugging',\n",
       " 'decode_base64',\n",
       " 'decode_compressed',\n",
       " 'decode_csv',\n",
       " 'decode_json_example',\n",
       " 'decode_raw',\n",
       " 'delete_session_tensor',\n",
       " 'depth_to_space',\n",
       " 'dequantize',\n",
       " 'deserialize_many_sparse',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_part',\n",
       " 'digamma',\n",
       " 'dimension_at_index',\n",
       " 'dimension_value',\n",
       " 'disable_eager_execution',\n",
       " 'disable_resource_variables',\n",
       " 'disable_v2_behavior',\n",
       " 'disable_v2_tensorshape',\n",
       " 'distribute',\n",
       " 'distributions',\n",
       " 'div',\n",
       " 'div_no_nan',\n",
       " 'divide',\n",
       " 'double',\n",
       " 'dtypes',\n",
       " 'dynamic_partition',\n",
       " 'dynamic_stitch',\n",
       " 'edit_distance',\n",
       " 'einsum',\n",
       " 'enable_eager_execution',\n",
       " 'enable_resource_variables',\n",
       " 'enable_v2_behavior',\n",
       " 'enable_v2_tensorshape',\n",
       " 'encode_base64',\n",
       " 'ensure_shape',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erfc',\n",
       " 'errors',\n",
       " 'estimator',\n",
       " 'examples',\n",
       " 'executing_eagerly',\n",
       " 'exp',\n",
       " 'expand_dims',\n",
       " 'experimental',\n",
       " 'expm1',\n",
       " 'extract_image_patches',\n",
       " 'extract_volume_patches',\n",
       " 'eye',\n",
       " 'fake_quant_with_min_max_args',\n",
       " 'fake_quant_with_min_max_args_gradient',\n",
       " 'fake_quant_with_min_max_vars',\n",
       " 'fake_quant_with_min_max_vars_gradient',\n",
       " 'fake_quant_with_min_max_vars_per_channel',\n",
       " 'fake_quant_with_min_max_vars_per_channel_gradient',\n",
       " 'feature_column',\n",
       " 'fft',\n",
       " 'fft2d',\n",
       " 'fft3d',\n",
       " 'fill',\n",
       " 'fixed_size_partitioner',\n",
       " 'flags',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'floor',\n",
       " 'floor_div',\n",
       " 'floordiv',\n",
       " 'floormod',\n",
       " 'foldl',\n",
       " 'foldr',\n",
       " 'gather',\n",
       " 'gather_nd',\n",
       " 'get_collection',\n",
       " 'get_collection_ref',\n",
       " 'get_default_graph',\n",
       " 'get_default_session',\n",
       " 'get_local_variable',\n",
       " 'get_logger',\n",
       " 'get_seed',\n",
       " 'get_session_handle',\n",
       " 'get_session_tensor',\n",
       " 'get_variable',\n",
       " 'get_variable_scope',\n",
       " 'gfile',\n",
       " 'global_norm',\n",
       " 'global_variables',\n",
       " 'global_variables_initializer',\n",
       " 'glorot_normal_initializer',\n",
       " 'glorot_uniform_initializer',\n",
       " 'gradients',\n",
       " 'graph_util',\n",
       " 'greater',\n",
       " 'greater_equal',\n",
       " 'group',\n",
       " 'guarantee_const',\n",
       " 'half',\n",
       " 'hessians',\n",
       " 'histogram_fixed_width',\n",
       " 'histogram_fixed_width_bins',\n",
       " 'identity',\n",
       " 'identity_n',\n",
       " 'ifft',\n",
       " 'ifft2d',\n",
       " 'ifft3d',\n",
       " 'igamma',\n",
       " 'igammac',\n",
       " 'imag',\n",
       " 'image',\n",
       " 'import_graph_def',\n",
       " 'init_scope',\n",
       " 'initialize_all_tables',\n",
       " 'initialize_all_variables',\n",
       " 'initialize_local_variables',\n",
       " 'initialize_variables',\n",
       " 'initializers',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'invert_permutation',\n",
       " 'io',\n",
       " 'is_finite',\n",
       " 'is_inf',\n",
       " 'is_nan',\n",
       " 'is_non_decreasing',\n",
       " 'is_numeric_tensor',\n",
       " 'is_strictly_increasing',\n",
       " 'is_variable_initialized',\n",
       " 'keras',\n",
       " 'layers',\n",
       " 'lbeta',\n",
       " 'less',\n",
       " 'less_equal',\n",
       " 'lgamma',\n",
       " 'lin_space',\n",
       " 'linalg',\n",
       " 'linspace',\n",
       " 'lite',\n",
       " 'load_file_system_library',\n",
       " 'load_library',\n",
       " 'load_op_library',\n",
       " 'local_variables',\n",
       " 'local_variables_initializer',\n",
       " 'log',\n",
       " 'log1p',\n",
       " 'log_sigmoid',\n",
       " 'logging',\n",
       " 'logical_and',\n",
       " 'logical_not',\n",
       " 'logical_or',\n",
       " 'logical_xor',\n",
       " 'losses',\n",
       " 'make_ndarray',\n",
       " 'make_template',\n",
       " 'make_tensor_proto',\n",
       " 'manip',\n",
       " 'map_fn',\n",
       " 'matching_files',\n",
       " 'math',\n",
       " 'matmul',\n",
       " 'matrix_band_part',\n",
       " 'matrix_determinant',\n",
       " 'matrix_diag',\n",
       " 'matrix_diag_part',\n",
       " 'matrix_inverse',\n",
       " 'matrix_set_diag',\n",
       " 'matrix_solve',\n",
       " 'matrix_solve_ls',\n",
       " 'matrix_square_root',\n",
       " 'matrix_transpose',\n",
       " 'matrix_triangular_solve',\n",
       " 'maximum',\n",
       " 'meshgrid',\n",
       " 'metrics',\n",
       " 'min_max_variable_partitioner',\n",
       " 'minimum',\n",
       " 'mod',\n",
       " 'model_variables',\n",
       " 'moving_average_variables',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'name_scope',\n",
       " 'negative',\n",
       " 'newaxis',\n",
       " 'nn',\n",
       " 'no_gradient',\n",
       " 'no_op',\n",
       " 'no_regularizer',\n",
       " 'norm',\n",
       " 'not_equal',\n",
       " 'one_hot',\n",
       " 'ones',\n",
       " 'ones_initializer',\n",
       " 'ones_like',\n",
       " 'op_scope',\n",
       " 'orthogonal_initializer',\n",
       " 'pad',\n",
       " 'parallel_stack',\n",
       " 'parse_example',\n",
       " 'parse_single_example',\n",
       " 'parse_single_sequence_example',\n",
       " 'parse_tensor',\n",
       " 'placeholder',\n",
       " 'placeholder_with_default',\n",
       " 'polygamma',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'profiler',\n",
       " 'py_func',\n",
       " 'py_function',\n",
       " 'python_io',\n",
       " 'pywrap_tensorflow',\n",
       " 'qint16',\n",
       " 'qint32',\n",
       " 'qint8',\n",
       " 'qr',\n",
       " 'quantization',\n",
       " 'quantize',\n",
       " 'quantize_v2',\n",
       " 'quantized_concat',\n",
       " 'quantized_conv2d_and_relu',\n",
       " 'quantized_conv2d_and_relu_and_requantize',\n",
       " 'quantized_conv2d_and_requantize',\n",
       " 'quantized_conv2d_with_bias',\n",
       " 'quantized_conv2d_with_bias_and_relu',\n",
       " 'quantized_conv2d_with_bias_and_relu_and_requantize',\n",
       " 'quantized_conv2d_with_bias_and_requantize',\n",
       " 'quantized_conv2d_with_bias_signed_sum_and_relu_and_requantize',\n",
       " 'quantized_conv2d_with_bias_sum_and_relu',\n",
       " 'quantized_conv2d_with_bias_sum_and_relu_and_requantize',\n",
       " 'queue',\n",
       " 'quint16',\n",
       " 'quint8',\n",
       " 'ragged',\n",
       " 'random',\n",
       " 'random_crop',\n",
       " 'random_gamma',\n",
       " 'random_normal',\n",
       " 'random_normal_initializer',\n",
       " 'random_poisson',\n",
       " 'random_shuffle',\n",
       " 'random_uniform',\n",
       " 'random_uniform_initializer',\n",
       " 'range',\n",
       " 'rank',\n",
       " 'read_file',\n",
       " 'real',\n",
       " 'realdiv',\n",
       " 'reciprocal',\n",
       " 'reduce_all',\n",
       " 'reduce_any',\n",
       " 'reduce_join',\n",
       " 'reduce_logsumexp',\n",
       " 'reduce_max',\n",
       " 'reduce_mean',\n",
       " 'reduce_min',\n",
       " 'reduce_prod',\n",
       " 'reduce_sum',\n",
       " 'regex_replace',\n",
       " 'register_tensor_conversion_function',\n",
       " 'report_uninitialized_variables',\n",
       " 'required_space_to_batch_paddings',\n",
       " 'reset_default_graph',\n",
       " 'reshape',\n",
       " 'resource',\n",
       " 'resource_loader',\n",
       " 'reverse',\n",
       " 'reverse_sequence',\n",
       " 'reverse_v2',\n",
       " 'rint',\n",
       " 'roll',\n",
       " 'round',\n",
       " 'rsqrt',\n",
       " 'saturate_cast',\n",
       " 'saved_model',\n",
       " 'scalar_mul',\n",
       " 'scan',\n",
       " 'scatter_add',\n",
       " 'scatter_div',\n",
       " 'scatter_max',\n",
       " 'scatter_min',\n",
       " 'scatter_mul',\n",
       " 'scatter_nd',\n",
       " 'scatter_nd_add',\n",
       " 'scatter_nd_sub',\n",
       " 'scatter_nd_update',\n",
       " 'scatter_sub',\n",
       " 'scatter_update',\n",
       " 'searchsorted',\n",
       " 'segment_max',\n",
       " 'segment_mean',\n",
       " 'segment_min',\n",
       " 'segment_prod',\n",
       " 'segment_sum',\n",
       " 'self_adjoint_eig',\n",
       " 'self_adjoint_eigvals',\n",
       " 'sequence_mask',\n",
       " 'serialize_many_sparse',\n",
       " 'serialize_sparse',\n",
       " 'serialize_tensor',\n",
       " 'set_random_seed',\n",
       " 'setdiff1d',\n",
       " 'sets',\n",
       " 'shape',\n",
       " 'shape_n',\n",
       " 'sigmoid',\n",
       " 'sign',\n",
       " 'signal',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'slice',\n",
       " 'sort',\n",
       " 'space_to_batch',\n",
       " 'space_to_batch_nd',\n",
       " 'space_to_depth',\n",
       " 'sparse',\n",
       " 'sparse_add',\n",
       " 'sparse_concat',\n",
       " 'sparse_fill_empty_rows',\n",
       " 'sparse_mask',\n",
       " 'sparse_matmul',\n",
       " 'sparse_maximum',\n",
       " 'sparse_merge',\n",
       " 'sparse_minimum',\n",
       " 'sparse_placeholder',\n",
       " 'sparse_reduce_max',\n",
       " 'sparse_reduce_max_sparse',\n",
       " 'sparse_reduce_sum',\n",
       " 'sparse_reduce_sum_sparse',\n",
       " 'sparse_reorder',\n",
       " 'sparse_reset_shape',\n",
       " 'sparse_reshape',\n",
       " 'sparse_retain',\n",
       " 'sparse_segment_mean',\n",
       " 'sparse_segment_sqrt_n',\n",
       " 'sparse_segment_sum',\n",
       " 'sparse_slice',\n",
       " 'sparse_softmax',\n",
       " 'sparse_split',\n",
       " 'sparse_tensor_dense_matmul',\n",
       " 'sparse_tensor_to_dense',\n",
       " 'sparse_to_dense',\n",
       " 'sparse_to_indicator',\n",
       " 'sparse_transpose',\n",
       " 'spectral',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squared_difference',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'stop_gradient',\n",
       " 'strided_slice',\n",
       " 'string',\n",
       " 'string_join',\n",
       " 'string_split',\n",
       " 'string_strip',\n",
       " 'string_to_hash_bucket',\n",
       " 'string_to_hash_bucket_fast',\n",
       " 'string_to_hash_bucket_strong',\n",
       " 'string_to_number',\n",
       " 'strings',\n",
       " 'substr',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'svd',\n",
       " 'sysconfig',\n",
       " 'tables_initializer',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tensor_scatter_add',\n",
       " 'tensor_scatter_sub',\n",
       " 'tensor_scatter_update',\n",
       " 'tensordot',\n",
       " 'test',\n",
       " 'tile',\n",
       " 'timestamp',\n",
       " 'to_bfloat16',\n",
       " 'to_complex128',\n",
       " 'to_complex64',\n",
       " 'to_double',\n",
       " 'to_float',\n",
       " 'to_int32',\n",
       " 'to_int64',\n",
       " 'tools',\n",
       " 'trace',\n",
       " 'train',\n",
       " 'trainable_variables',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncated_normal',\n",
       " 'truncated_normal_initializer',\n",
       " 'truncatediv',\n",
       " 'truncatemod',\n",
       " 'tuple',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'uniform_unit_scaling_initializer',\n",
       " 'unique',\n",
       " 'unique_with_counts',\n",
       " 'unravel_index',\n",
       " 'unsorted_segment_max',\n",
       " 'unsorted_segment_mean',\n",
       " 'unsorted_segment_min',\n",
       " 'unsorted_segment_prod',\n",
       " 'unsorted_segment_sqrt_n',\n",
       " 'unsorted_segment_sum',\n",
       " 'unstack',\n",
       " 'user_ops',\n",
       " 'variable_axis_size_partitioner',\n",
       " 'variable_creator_scope',\n",
       " 'variable_op_scope',\n",
       " 'variable_scope',\n",
       " 'variables_initializer',\n",
       " 'variance_scaling_initializer',\n",
       " 'variant',\n",
       " 'verify_tensor_all_finite',\n",
       " 'version',\n",
       " 'where',\n",
       " 'while_loop',\n",
       " 'wrap_function',\n",
       " 'write_file',\n",
       " 'zeros',\n",
       " 'zeros_initializer',\n",
       " 'zeros_like',\n",
       " 'zeta']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goseek",
   "language": "python",
   "name": "goseek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
