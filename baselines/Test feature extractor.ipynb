{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../NeuralTuringMachine')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from ntm import NTMCell\n",
    "\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-33b93ad34dd7>:116: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "inputs (?, 1, 256)\n",
      "WARNING:tensorflow:From ../../NeuralTuringMachine/ntm.py:31: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From ../../NeuralTuringMachine/ntm.py:33: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-33b93ad34dd7>:76: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-33b93ad34dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dropout (keep probability)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn1_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn2_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-33b93ad34dd7>\u001b[0m in \u001b[0;36mcnn\u001b[0;34m(X, num_classes, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;31m#     g1, attn1_layer = attention_block(conv1, g, 'attn1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m#     g1 = conv_to_fc(g1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-33b93ad34dd7>\u001b[0m in \u001b[0;36mmemory_module\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     output_sequence, _ = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, time_major=False, sequence_length=[1]*128,\n\u001b[0;32m---> 76\u001b[0;31m                                            dtype=tf.float32, initial_state=None)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_sequence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_initial_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         state = cell.get_initial_state(\n\u001b[0;32m--> 643\u001b[0;31m             inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[0m\u001b[1;32m    644\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0;34m\"batch_size and dtype cannot be None while constructing initial \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m           \"state: batch_size={}, dtype={}\".format(batch_size, dtype))\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspaces/NeuralTuringMachine/ntm.py\u001b[0m in \u001b[0;36mzero_state\u001b[0;34m(self, batch_size, dtype)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             read_vector_list = [expand(tf.tanh(learned_init(self.memory_vector_dim)), dim=0, N=batch_size)\n\u001b[0;32m--> 152\u001b[0;31m                 for i in range(self.read_head_num)]\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             w_list = [expand(tf.nn.softmax(learned_init(self.memory_size)), dim=0, N=batch_size)\n",
      "\u001b[0;32m~/workspaces/NeuralTuringMachine/ntm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             read_vector_list = [expand(tf.tanh(learned_init(self.memory_vector_dim)), dim=0, N=batch_size)\n\u001b[0;32m--> 152\u001b[0;31m                 for i in range(self.read_head_num)]\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             w_list = [expand(tf.nn.softmax(learned_init(self.memory_size)), dim=0, N=batch_size)\n",
      "\u001b[0;32m~/workspaces/NeuralTuringMachine/utils.py\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(x, dim, N)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlearned_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def conv_to_fc(input_tensor):\n",
    "    n_hidden = tf.reduce_prod([v.value for v in input_tensor.get_shape()[1:]])\n",
    "    input_tensor = tf.reshape(input_tensor, [-1, n_hidden])\n",
    "    return input_tensor\n",
    "\n",
    "def linear(input_tensor, num_hidden, scope):\n",
    "    n_input = input_tensor.get_shape()[1].value\n",
    "    with tf.variable_scope(scope):\n",
    "        weight = tf.get_variable(\"w\", [n_input, num_hidden], initializer=tf.initializers.orthogonal())\n",
    "        bias = tf.get_variable(\"b\", [num_hidden], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input_tensor, weight) + bias\n",
    "    \n",
    "def linear2d(input_tensor, num_hidden, scope):\n",
    "    b, h, w = input_tensor.shape\n",
    "    with tf.variable_scope(scope):\n",
    "        tensors = []\n",
    "        for i in range(h):\n",
    "            weight = tf.get_variable(\"w\"+str(i), [w, num_hidden], initializer=tf.initializers.orthogonal())\n",
    "            bias = tf.get_variable(\"b\"+str(i), [num_hidden], initializer=tf.constant_initializer(0.0))\n",
    "            tensors.append(tf.matmul(input_tensor[:,i,:], weight) + bias)\n",
    "        return tf.stack(tensors, axis=1)\n",
    "    \n",
    "    \n",
    "def conv2d(input_tensor, num_filters, filter_size, stride, scope,  **kwargs):\n",
    "    num_channels = input_tensor.get_shape()[-1].value\n",
    "    filter_height = filter_width = filter_size\n",
    "    wshape = [filter_height, filter_width, num_channels, num_filters]\n",
    "    bshape = [1, 1, 1, num_filters]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    with tf.variable_scope(scope):\n",
    "        weights = tf.get_variable(\"w\", wshape, initializer=tf.initializers.orthogonal())\n",
    "        bias = tf.get_variable(\"b\", bshape, initializer=tf.constant_initializer(0.0))\n",
    "        return bias + tf.nn.conv2d(input_tensor, weights, strides=strides, padding='VALID', data_format='NHWC',  **kwargs)\n",
    "\n",
    "def softmax_2d(tensor):\n",
    "    b, h, w, c = tensor.shape\n",
    "    tensor = tf.reshape(tensor, (-1, h * w, c))\n",
    "    tensor = tf.nn.softmax(tensor, axis=1)\n",
    "    tensor = tf.reshape(tensor, (-1, h, w, c))\n",
    "    return tensor\n",
    "    \n",
    "def attention_block(tensor, g, scope):\n",
    "    b, h, w, f = tensor.shape\n",
    "    ls = tf.reshape(tensor, (-1, h*w, f))\n",
    "    print(\"ls\",ls.get_shape())\n",
    "    g_size = g.get_shape()[-1].value\n",
    "    print(\"g\", g.get_shape())\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        lsat = linear2d(ls, num_hidden=g_size, scope='lsat') # (-1, h*w, g_size)\n",
    "        lsat = tf.nn.relu(lsat)\n",
    "        print(\"lsat\", lsat.get_shape())\n",
    "        ### TODO is including also the batch dimension correct? ###\n",
    "        g_tiled = tf.tile(tf.reshape(g, (-1, 1, g_size)), [1, h*w, 1])\n",
    "        compatibility = tf.reduce_sum(tf.multiply(lsat, g_tiled), axis=-1, keepdims=True) #tf.tensordot(lsat, g_tiled, axes=((-1), (-1))) # (-1, h*w, 1)\n",
    "#         compatibility = tf.reshape(compatibility, shape=[-1, h*w, 1]) # (-1, h*w)\n",
    "        print(\"compatibility\", compatibility.get_shape())\n",
    "        attention = tf.nn.softmax(compatibility, axis=1) # (-1, h*w)\n",
    "    #     attention = tf.tile(tf.reshape(attention, shape=(-1, h*w, 1)), [1, 1, f]) # (-1, h*w, f)\n",
    "        attention_tiled = tf.tile(attention, [1, 1, f]) # (-1, h*w, f)\n",
    "        print(\"attention\", attention_tiled.get_shape())\n",
    "        weighted_ls = attention_tiled * ls\n",
    "        return weighted_ls, attention\n",
    "    \n",
    "def memory_module(inputs):\n",
    "#     inputs_placeholder = tf.placeholder(tf.float32, shape=(args.batch_size, None, args.num_bits_per_vector+1))\n",
    "#     outputs_placeholder = tf.placeholder(tf.float32, shape=(args.batch_size, None, args.num_bits_per_vector))\n",
    "    _, l = inputs.get_shape()\n",
    "    inputs = tf.reshape(inputs, (-1, 1, l))\n",
    "    print(\"inputs\", inputs.get_shape())\n",
    "    cell = NTMCell(controller_layers=2, controller_units=100, memory_size=128, memory_vector_dim=12, \n",
    "                   read_head_num=1, write_head_num=1, addressing_mode='content_and_location', \n",
    "                   shift_range=1, reuse=False, output_dim=None, clip_value=20, init_mode='constant')\n",
    "    \n",
    "    output_sequence, _ = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, time_major=False,\n",
    "                                           dtype=tf.float32, initial_state=None)\n",
    "    \n",
    "    print(\"output_sequence\", output_sequence.get_shape())\n",
    "    outputs = tf.sigmoid(output_sequence)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "def cnn(X, num_classes, **kwargs):\n",
    "    batch_size = X.shape[0].value\n",
    "    scaled_images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = conv2d(input_tensor=scaled_images, num_filters=32, filter_size=2, stride=2, scope=\"conv1\")\n",
    "    conv1 = tf.nn.elu(conv1)\n",
    "#     conv1 = tf.nn.l2_normalize(conv1, axis=-1)\n",
    "    conv2 = conv2d(input_tensor=conv1, num_filters=64, filter_size=2, stride=2, scope=\"conv2\")\n",
    "    conv2 = tf.nn.elu(conv2)\n",
    "#     conv2 = tf.nn.l2_normalize(conv2, axis=-1)\n",
    "    \n",
    "    g = tf.nn.relu(linear(conv_to_fc(conv2), num_hidden=256, scope=\"ln1\"))\n",
    "    \n",
    "\n",
    "    lst = memory_module(g)\n",
    "#     g1, attn1_layer = attention_block(conv1, g, 'attn1') \n",
    "#     g1 = conv_to_fc(g1)\n",
    "#     g2, attn2_layer = attention_block(conv2, g, 'attn2') \n",
    "#     g2 = conv_to_fc(g2)\n",
    "    \n",
    "#     last = tf.concat((g1, g2), 1)\n",
    "    \n",
    "    ln1 = linear(last, num_hidden=num_classes, scope=\"lnlast\")\n",
    "    \n",
    "    return tf.nn.relu(ln1)#, attn1_layer, attn2_layer\n",
    "\n",
    "\n",
    "if 'sess' in locals():\n",
    "    sess.close()\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.0001\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "\n",
    "logits, attn1_layer, attn2_layer = cnn(X, num_classes)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1, num_steps+1):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                             Y: batch_y,\n",
    "                                                             keep_prob: 1.0})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Calculate accuracy for 256 MNIST test images\n",
    "print(\"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                  Y: mnist.test.labels[:256],\n",
    "                                  keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_img = mnist.test.images[270:300]\n",
    "test_y = mnist.test.labels[270:300]\n",
    "\n",
    "res_op = tf.argmax(prediction, 1)\n",
    "\n",
    "    \n",
    "result = sess.run(res_op, feed_dict={X: np.reshape(test_img, (test_img.shape[0], -1))})\n",
    "print(result, \"==\\n\", np.argmax(test_y, 1), \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### net visualization\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "extent = 0, 28, 0, 28\n",
    "\n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(1, figsize=(10,10))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\")\n",
    "        \n",
    "def plotAttention(units):\n",
    "    b, px, rest = units.shape\n",
    "    print(np.amax(units), np.amin(units))\n",
    "#     plt.figure(1, figsize=(20,20))\n",
    "    h = w = int(math.sqrt(px))\n",
    "    print(h, w)\n",
    "    units = np.reshape(units, (h, w))\n",
    "    plt.imshow(units, interpolation=\"nearest\", cmap=\"viridis\", alpha=0.5, extent=extent)\n",
    "\n",
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={X:np.reshape(stimuli,[1,784],order='F'),keep_prob:1.0})\n",
    "    plotAttention(units)\n",
    "\n",
    "    \n",
    "fig = plt.figure(frameon=True)\n",
    "\n",
    "imageToUse = mnist.test.images[np.random.randint(low=0, high=len(mnist.test.images))]\n",
    "plt.imshow(np.reshape(imageToUse,[28,28]), interpolation=\"nearest\", cmap=\"gray\", extent=extent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "getActivations(attn1_layer, imageToUse)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:goseek] *",
   "language": "python",
   "name": "conda-env-goseek-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
