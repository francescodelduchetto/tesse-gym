{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Distribution authorized to U.S. Government agencies and their contractors. Other requests for this document shall be referred to the MIT Lincoln Laboratory Technology Office.\n",
    "\n",
    "This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n",
    "\n",
    "Â© 2019 Massachusetts Institute of Technology.\n",
    "\n",
    "The software/firmware is provided to you on an As-Is basis\n",
    "\n",
    "Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treasure Hunt Challenge\n",
    "\n",
    "This notebook uses [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) to train an agent for the [GOSEEK-Challenge](https://github.mit.edu/TESS/goseek-challenge). \n",
    "\n",
    "Proximal Policy Optimization is used to train an agent defined by a CNN-LSTM network. The agent's observations consist of RGB, segmentation, and depth images and relative pose. This, along with the reward function, is defined in the [GoSeekFullPerception](https://github.mit.edu/TESS/tesse-gym/blob/master/src/tesse_gym/tasks/goseek/goseek_full_perception.py#L30) [gym environment](https://gym.openai.com/). \n",
    "\n",
    "\n",
    "__Contents__\n",
    "- [Configure Environment](#Configuration)\n",
    "- [Define Model](#Define-the-Model)\n",
    "- [Train Model](#Train-the-Model)\n",
    "- [Visualize Results](#Visualize-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines import A2C\n",
    "from tesse.msgs import *\n",
    "\n",
    "from tesse_gym import get_network_config\n",
    "from tesse_gym.tasks.goseek import GoSeekFullPerception, decode_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set sim path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\"../../goseek-challenge/simulator/goseek-v0.1.4.x86_64\")\n",
    "assert filename.exists(), f\"Must set a valid path!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment parameters\n",
    "\n",
    "\n",
    "__Note__ To minimize training time during initial use, we've set `total_timestamps` and `n_environments` to 1e5 and 2 respectively. Setting `total_timestamps` to 3e6 and `n_environments` to 4 should produce an agent that approximates our baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 5  # number of environments to train over\n",
    "total_timesteps = 500001  # number of training timesteps\n",
    "scene_id = [1, 2, 3, 4, 5]  # list all available scenes\n",
    "n_targets = 30  # number of targets spawned in each scene\n",
    "target_found_reward = 2  # reward per found target\n",
    "episode_length = 400\n",
    "\n",
    "\n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = GoSeekFullPerception(\n",
    "                str(filename),\n",
    "                network_config=get_network_config(worker_id=rank),\n",
    "                n_targets=n_targets,\n",
    "                episode_length=episode_length,\n",
    "                scene_id=scene_id[rank],#np.random.choice(scene_id),\n",
    "                target_found_reward=target_found_reward,\n",
    "            )\n",
    "            return env\n",
    "\n",
    "        return _thunk\n",
    "\n",
    "    return SubprocVecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model \n",
    "\n",
    "The following network assumes an observation of consisting of RGB, segmentation, and depth images along with the agent's relative pose from start. Images are processed using the Stable Baseline default CNN. The resulting feature vector is concatenated with the pose vector and given to an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network to consume images and pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tensor_observations(observation, img_shape=(-1, 240, 320, 5)):\n",
    "    \"\"\" Decode observation vector into images and poses.\n",
    "\n",
    "    Args:\n",
    "        observation (np.ndarray): Shape (N,) observation array of flattened\n",
    "            images concatenated with a pose vector. Thus, N is equal to N*H*W*C + N*3.\n",
    "        img_shape (Tuple[int, int, int, int]): Shapes of all images stacked in (N, H, W, C).\n",
    "            Default value is (-1, 240, 320, 5).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor]: Tensors with the following information\n",
    "            - Tensor of shape (N, `img_shape[1:]`) containing RGB,\n",
    "                segmentation, and depth images stacked across the channel dimension.\n",
    "            - Tensor of shape (N, 3) containing (x, y, heading) relative to starting point.\n",
    "                (x, y) are in meters, heading is given in degrees in the range [-180, 180].\n",
    "    \"\"\"\n",
    "    \n",
    "    imgs = tf.reshape(observation[:, :-3], img_shape)\n",
    "    im1 = tf.image.resize(\n",
    "        imgs[..., :3], tf.constant([img_shape[1]//10, img_shape[2]//10], dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    im2 = tf.image.resize(\n",
    "        tf.expand_dims(imgs[..., 3], axis=3), tf.constant([img_shape[1]//10, img_shape[2]//10], dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    im3 = tf.image.resize(\n",
    "        tf.expand_dims(imgs[..., 4], axis=3), tf.constant([img_shape[1]//10, img_shape[2]//10], dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "\n",
    "    new_imgs = im2 #tf.concat([im1, im2, im3], axis=3)\n",
    "    pose = observation[:, -3:]\n",
    "\n",
    "    return tf.reshape(new_imgs, [-1, new_imgs.shape[1]*new_imgs.shape[2]*new_imgs.shape[3]]), imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_pose_network(observation, **kwargs):\n",
    "    \"\"\" Network to process image and pose data.\n",
    "    \n",
    "    Use the stable baselines nature_cnn to process images. The resulting\n",
    "    feature vector is then combined with the pose estimate and given to an\n",
    "    LSTM (LSTM defined in PPO2 below).\n",
    "    \n",
    "    Args:\n",
    "        raw_observations (tf.Tensor): 1D tensor containing image and \n",
    "            pose data.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Feature vector. \n",
    "    \"\"\"\n",
    "    imgs, orig_imgs, pose = decode_tensor_observations(observation)\n",
    "    image_features = nature_cnn(orig_imgs)\n",
    "    print(imgs.shape, pose.shape)\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register custom network\n",
    "\n",
    "Outputs of the network defined above will be fed into an LSTM defined below in PPO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = {'cnn_extractor': image_and_pose_network}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(5, 768) (5, 3)\n",
      "(25, 768) (25, 3)\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\n",
    "    CnnLstmPolicy,\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    gamma=0.995,\n",
    "    learning_rate=0.00025,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"results/goseek-a2c-naturecnn\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "total_updates = 0\n",
    "def save_checkpoint_callback(local_vars, global_vars):\n",
    "    global total_updates\n",
    "#     print(f\"=== local vars ===\\n{local_vars.keys()}\")  # add this line \n",
    "#     total_updates = local_vars[\"n_updates\"]\n",
    "    total_updates += 1\n",
    "    if total_updates % 1000 == 0:\n",
    "        local_vars[\"self\"].save(str(log_dir / f\"{total_updates:06d}.pkl\"))\n",
    "        print(\"Saving model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | 0.0555   |\n",
      "| fps                | 2        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 1.39     |\n",
      "| total_timesteps    | 25       |\n",
      "| value_loss         | 0.142    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.312    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 1.38     |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 0.00712  |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.878    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 1.38     |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 0.00593  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.972    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 1.38     |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 0.00359  |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.982    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 1.38     |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 0.00391  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.998    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 1.37     |\n",
      "| total_timesteps    | 12500    |\n",
      "| value_loss         | 0.00209  |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.998    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 1.33     |\n",
      "| total_timesteps    | 15000    |\n",
      "| value_loss         | 0.000837 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.961    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 1.27     |\n",
      "| total_timesteps    | 17500    |\n",
      "| value_loss         | 0.0058   |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | -0.0339  |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 1.11     |\n",
      "| total_timesteps    | 20000    |\n",
      "| value_loss         | 0.374    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.932    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.925    |\n",
      "| total_timesteps    | 22500    |\n",
      "| value_loss         | 0.00173  |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.993    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 1.06     |\n",
      "| total_timesteps    | 25000    |\n",
      "| value_loss         | 0.000431 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.972    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.827    |\n",
      "| total_timesteps    | 27500    |\n",
      "| value_loss         | 0.000198 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.914    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.986    |\n",
      "| total_timesteps    | 30000    |\n",
      "| value_loss         | 0.0016   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.982    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 1.08     |\n",
      "| total_timesteps    | 32500    |\n",
      "| value_loss         | 0.00021  |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.131    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 1.02     |\n",
      "| total_timesteps    | 35000    |\n",
      "| value_loss         | 0.161    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.976    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 1.13     |\n",
      "| total_timesteps    | 37500    |\n",
      "| value_loss         | 0.000471 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.916    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 1.18     |\n",
      "| total_timesteps    | 40000    |\n",
      "| value_loss         | 0.000432 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.931    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.954    |\n",
      "| total_timesteps    | 42500    |\n",
      "| value_loss         | 0.000574 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.905    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 1.01     |\n",
      "| total_timesteps    | 45000    |\n",
      "| value_loss         | 0.000391 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.941    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 1.16     |\n",
      "| total_timesteps    | 47500    |\n",
      "| value_loss         | 0.000719 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.982    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 1.15     |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 0.000292 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.821    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 1.06     |\n",
      "| total_timesteps    | 52500    |\n",
      "| value_loss         | 0.000699 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.716    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2200     |\n",
      "| policy_entropy     | 1.04     |\n",
      "| total_timesteps    | 55000    |\n",
      "| value_loss         | 0.000326 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.191    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2300     |\n",
      "| policy_entropy     | 1.01     |\n",
      "| total_timesteps    | 57500    |\n",
      "| value_loss         | 0.000328 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | -0.00112 |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2400     |\n",
      "| policy_entropy     | 0.911    |\n",
      "| total_timesteps    | 60000    |\n",
      "| value_loss         | 0.000471 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.241    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2500     |\n",
      "| policy_entropy     | 1.11     |\n",
      "| total_timesteps    | 62500    |\n",
      "| value_loss         | 0.777    |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.869    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2600     |\n",
      "| policy_entropy     | 1.01     |\n",
      "| total_timesteps    | 65000    |\n",
      "| value_loss         | 0.000931 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.937    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2700     |\n",
      "| policy_entropy     | 0.883    |\n",
      "| total_timesteps    | 67500    |\n",
      "| value_loss         | 0.000234 |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.472    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 1.1      |\n",
      "| total_timesteps    | 70000    |\n",
      "| value_loss         | 0.000676 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.975    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 1.09     |\n",
      "| total_timesteps    | 72500    |\n",
      "| value_loss         | 0.0013   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.975    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 1.06     |\n",
      "| total_timesteps    | 75000    |\n",
      "| value_loss         | 0.000529 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.985    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 0.957    |\n",
      "| total_timesteps    | 77500    |\n",
      "| value_loss         | 0.00019  |\n",
      "---------------------------------\n",
      "Saving model\n",
      "---------------------------------\n",
      "| explained_variance | 0.976    |\n",
      "| fps                | 6        |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 1.08     |\n",
      "| total_timesteps    | 80000    |\n",
      "| value_loss         | 0.000369 |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a8bc6236ba26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_checkpoint_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mep_info_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 _, value_loss, policy_entropy = self._train_step(obs, states, rewards, masks, actions, values,\n\u001b[0;32m--> 273\u001b[0;31m                                                                  self.num_timesteps // self.n_batch, writer)\n\u001b[0m\u001b[1;32m    274\u001b[0m                 \u001b[0mn_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/a2c/a2c.py\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, obs, states, rewards, masks, actions, values, update, writer)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 summary, policy_loss, value_loss, policy_entropy, _ = self.sess.run(\n\u001b[0;32m--> 233\u001b[0;31m                     [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.apply_backprop], td_map)\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=total_timesteps, callback=save_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(log_dir) +\"/final_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "\n",
    "__Note__: Stable-Baselines requires that policy input dimensions be consistent across training and testing. Thus, the number of environments used for visualization must be a multiple of the number of environments used for training. The observation vector is then appropriately duplicated during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS_PATH = \"results/goseek-a2c/final_model.pkl\"\n",
    "assert MODEL_WEIGHTS_PATH, f\"Must give a model weights path!\"\n",
    "\n",
    "model = A2C.load(str(MODEL_WEIGHTS_PATH))\n",
    "# print(dir(model))\n",
    "n_train_envs = model.n_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize all observed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None\n",
    "\n",
    "print(pose)\n",
    "\n",
    "assert (\n",
    "    n_train_envs % obs.shape[0] == 0\n",
    "), f\"The number of visualization environments must be a multiple of the training environments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(rgb[0])\n",
    "ax[1].imshow(segmentation[0])\n",
    "ax[2].imshow(depth[0])\n",
    "\n",
    "print(rgb[0].shape)\n",
    "print(segmentation[0].shape)\n",
    "print(depth[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an episode and plot the first person agent view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - check that the state is correct (segmentation and values of classes)\n",
    "# - check the robot pose values\n",
    "done = False\n",
    "fig, ax = plt.subplots(1, obs.shape[0])\n",
    "ax = [ax] if obs.shape[0] == 1 else ax\n",
    "\n",
    "for i in range(episode_length):\n",
    "    actions, lstm_state = model.predict(\n",
    "        np.concatenate((n_train_envs // obs.shape[0]) * [obs]),\n",
    "        state=lstm_state,\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    actions = actions[: obs.shape[0]]\n",
    "\n",
    "    obs, reward, done, _ = env.step(actions)\n",
    "#     print(actions, done, reward)\n",
    "\n",
    "    plt.cla()\n",
    "    rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "    \n",
    "\n",
    "    for i in range(obs.shape[0]):\n",
    "#         print(segmentation[i][:,10])\n",
    "        ax[i].imshow(segmentation[i][::10,::10], vmin=0., vmax=1.)\n",
    "#         ax[i].imshow(segmentation[i])\n",
    "#         print(segmentation[i][::10,::10])\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:goseek] *",
   "language": "python",
   "name": "conda-env-goseek-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
