{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Distribution authorized to U.S. Government agencies and their contractors. Other requests for this document shall be referred to the MIT Lincoln Laboratory Technology Office.\n",
    "\n",
    "This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n",
    "\n",
    "Â© 2019 Massachusetts Institute of Technology.\n",
    "\n",
    "The software/firmware is provided to you on an As-Is basis\n",
    "\n",
    "Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treasure Hunt Challenge\n",
    "\n",
    "This notebook uses [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) to train an agent for the [GOSEEK-Challenge](https://github.mit.edu/TESS/goseek-challenge). \n",
    "\n",
    "Proximal Policy Optimization is used to train an agent defined by a CNN-LSTM network. The agent's observations consist of RGB, segmentation, and depth images and relative pose. This, along with the reward function, is defined in the [GoSeekFullPerception](https://github.mit.edu/TESS/tesse-gym/blob/master/src/tesse_gym/tasks/goseek/goseek_full_perception.py#L30) [gym environment](https://gym.openai.com/). \n",
    "\n",
    "\n",
    "__Contents__\n",
    "- [Configure Environment](#Configuration)\n",
    "- [Define Model](#Define-the-Model)\n",
    "- [Train Model](#Train-the-Model)\n",
    "- [Visualize Results](#Visualize-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from tesse.msgs import *\n",
    "\n",
    "from tesse_gym import get_network_config\n",
    "from tesse_gym.tasks.goseek import GoSeekFullPerception, decode_observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set sim path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\"../../goseek-challenge/simulator/goseek-v0.1.4.x86_64\")\n",
    "assert filename.exists(), f\"Must set a valid path!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment parameters\n",
    "\n",
    "\n",
    "__Note__ To minimize training time during initial use, we've set `total_timestamps` and `n_environments` to 1e5 and 2 respectively. Setting `total_timestamps` to 3e6 and `n_environments` to 4 should produce an agent that approximates our baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 5  # number of environments to train over\n",
    "total_timesteps = 1000001  # number of training timesteps\n",
    "scene_id = [1, 2, 3, 4, 5]  # list all available scenes\n",
    "n_targets = 30  # number of targets spawned in each scene\n",
    "target_found_reward = 2  # reward per found target\n",
    "episode_length = 400\n",
    "\n",
    "\n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = GoSeekFullPerception(\n",
    "                str(filename),\n",
    "                network_config=get_network_config(worker_id=rank),\n",
    "                n_targets=n_targets,\n",
    "                episode_length=episode_length,\n",
    "                scene_id=scene_id[rank%len(scene_id)],#np.random.choice(scene_id),\n",
    "                target_found_reward=target_found_reward,\n",
    "            )\n",
    "            return env\n",
    "\n",
    "        return _thunk\n",
    "\n",
    "    return SubprocVecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)\n",
    "\n",
    "LOAD_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model \n",
    "\n",
    "The following network assumes an observation of consisting of RGB, segmentation, and depth images along with the agent's relative pose from start. Images are processed using the Stable Baseline default CNN. The resulting feature vector is concatenated with the pose vector and given to an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network to consume images and pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_tensor_observations(observation, img_shape=(-1, 240, 320, 5)):\n",
    "    \"\"\" Decode observation vector into images and poses.\n",
    "\n",
    "    Args:\n",
    "        observation (np.ndarray): Shape (N,) observation array of flattened\n",
    "            images concatenated with a pose vector. Thus, N is equal to N*H*W*C + N*3.\n",
    "        img_shape (Tuple[int, int, int, int]): Shapes of all images stacked in (N, H, W, C).\n",
    "            Default value is (-1, 240, 320, 5).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor]: Tensors with the following information\n",
    "            - Tensor of shape (N, `img_shape[1:]`) containing RGB,\n",
    "                segmentation, and depth images stacked across the channel dimension.\n",
    "            - Tensor of shape (N, 3) containing (x, y, heading) relative to starting point.\n",
    "                (x, y) are in meters, heading is given in degrees in the range [-180, 180].\n",
    "    \"\"\"\n",
    "    \n",
    "    imgs = tf.reshape(observation[:, :-3], img_shape)[..., -2:]\n",
    "    pose = observation[:, -3:]\n",
    "#     im1 = tf.image.resize(\n",
    "#         imgs[..., :3], tf.constant([img_shape[1]//10, img_shape[2]//10], dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "#     )\n",
    "#     im2 = tf.image.resize(\n",
    "#         tf.expand_dims(imgs[..., 3], axis=3), tf.constant([img_shape[1]//10, img_shape[2]//10], dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "#     )\n",
    "#     im3 = tf.image.resize(\n",
    "#         tf.expand_dims(imgs[..., 4], axis=3), tf.constant([img_shape[1]//10, img_shape[2]//10], dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "#     )\n",
    "\n",
    "#     new_imgs = im2 #tf.concat([im1, im2, im3], axis=3)\n",
    "\n",
    "\n",
    "#     return tf.reshape(new_imgs, [-1, new_imgs.shape[1]*new_imgs.shape[2]*new_imgs.shape[3]]), imgs, pose\n",
    "    return imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "\n",
    "def cnn(scaled_images, **kwargs):\n",
    "\n",
    "    c1 = tf.nn.relu(conv(scaled_images, 'c1', n_filters=16, filter_size=4, stride=2, init_scale=np.sqrt(2), **kwargs))\n",
    "    c2 = tf.nn.relu(conv(c1, 'c2', n_filters=24, filter_size=4, stride=2, init_scale=np.sqrt(2), **kwargs))\n",
    "    c3 = tf.nn.relu(conv(c2, 'c3', n_filters=32, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "#     c3 = tf.nn.l2_normalize(c3, axis=-1)\n",
    "\n",
    "    g = tf.nn.relu(linear(conv_to_fc(c3), n_hidden=128, scope=\"g\", init_scale=np.sqrt(2)))\n",
    "    \n",
    "    return g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_combiner(image_features, pose):\n",
    "    _, plen = pose.get_shape()\n",
    "    pose_rs = tf.reshape(pose, (-1, 1, plen))\n",
    "    _, featlen = image_features.get_shape()\n",
    "    image_features_rs = tf.reshape(image_features, (-1, 1, featlen))\n",
    "    \n",
    "    ## memory for pose\n",
    "    (pose_output, _, _) = tf.compat.v1.keras.layers.CuDNNLSTM(units=64, stateful=True, return_state=True)(pose_rs)\n",
    "    print(\"pose_output\", pose_output.get_shape())\n",
    "    \n",
    "    ## memory for image_features\n",
    "    (img_output, _, _) = tf.compat.v1.keras.layers.CuDNNLSTM(units=256, stateful=True, return_state=True)(image_features_rs)\n",
    "    print(\"img_output\", img_output.get_shape())\n",
    "    \n",
    "    ## combined memory\n",
    "    \n",
    "    return  tf.concat((image_features, img_output, pose, pose_output), axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_pose_network(observation, **kwargs):\n",
    "    \"\"\" Network to process image and pose data.\n",
    "    \n",
    "    Use the stable baselines nature_cnn to process images. The resulting\n",
    "    feature vector is then combined with the pose estimate and given to an\n",
    "    LSTM (LSTM defined in PPO2 below).\n",
    "    \n",
    "    Args:\n",
    "        raw_observations (tf.Tensor): 1D tensor containing image and \n",
    "            pose data.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Feature vector. \n",
    "    \"\"\"\n",
    "    orig_imgs, pose = decode_tensor_observations(observation)\n",
    "    scaled_imgs = tf.image.resize_images(orig_imgs, [40, 40], method=1) # 1: nearest\n",
    "    image_features = cnn(scaled_imgs)\n",
    "    combined_features = features_combiner(image_features, pose)\n",
    "#     print(image_features.shape, imgs.shape, pose.shape)\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register custom network\n",
    "\n",
    "Outputs of the network defined above will be fed into an LSTM defined below in PPO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_kwargs = {'cnn_extractor': image_and_pose_network}\n",
    "policy_kwargs = {'cnn_extractor': image_and_pose_network}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "pose_output (5, 64)\n",
      "img_output (5, 256)\n",
      "pose_output (128, 64)\n",
      "img_output (128, 256)\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/francesco/anaconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if LOAD_MODEL:\n",
    "    MODEL_WEIGHTS_PATH = \"results/goseek-ppo-lstm-forwardreward/final_model.pkl\"\n",
    "    assert MODEL_WEIGHTS_PATH, f\"Must give a model weights path!\"\n",
    "    model = PPO2.load(str(MODEL_WEIGHTS_PATH),env=env, tensorboard_log=\"./tensorboard/\", gamma=0.995, learning_rate=0.0002)\n",
    "else:\n",
    "    model = PPO2(\n",
    "        CnnLstmPolicy,\n",
    "        env,\n",
    "    #     n_steps=100,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tensorboard/\",\n",
    "        nminibatches=5,\n",
    "        gamma=0.995,\n",
    "        learning_rate=0.00025,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"results/goseek-ppo-lstm-forwardreward-retrain\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "total_updates = 0\n",
    "def save_checkpoint_callback(local_vars, global_vars):\n",
    "    global total_updates\n",
    "#     print(f\"=== local vars ===\\n{local_vars.keys()}\")  # add this line \n",
    "#     total_updates = local_vars[\"n_updates\"]\n",
    "    total_updates += 1\n",
    "    if total_updates % 1000 == 0:\n",
    "        local_vars[\"self\"].save(str(log_dir / f\"{total_updates:06d}.pkl\"))\n",
    "        print(\"Saving model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| approxkl           | 0.0046764873 |\n",
      "| clipfrac           | 0.044140626  |\n",
      "| explained_variance | 0.532        |\n",
      "| fps                | 11           |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 1.0203629    |\n",
      "| policy_loss        | -0.0141234   |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 0.000147     |\n",
      "| total_timesteps    | 640          |\n",
      "| value_loss         | 0.5051893    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0057010315 |\n",
      "| clipfrac           | 0.0625       |\n",
      "| explained_variance | 0.668        |\n",
      "| fps                | 22           |\n",
      "| n_updates          | 2            |\n",
      "| policy_entropy     | 0.8271537    |\n",
      "| policy_loss        | -0.015637541 |\n",
      "| serial_timesteps   | 256          |\n",
      "| time_elapsed       | 56.4         |\n",
      "| total_timesteps    | 1280         |\n",
      "| value_loss         | 0.45759135   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0075664506 |\n",
      "| clipfrac           | 0.10429688   |\n",
      "| explained_variance | 0.942        |\n",
      "| fps                | 25           |\n",
      "| n_updates          | 3            |\n",
      "| policy_entropy     | 0.9219788    |\n",
      "| policy_loss        | -0.016781202 |\n",
      "| serial_timesteps   | 384          |\n",
      "| time_elapsed       | 85.3         |\n",
      "| total_timesteps    | 1920         |\n",
      "| value_loss         | 0.029409086  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0063012987 |\n",
      "| clipfrac           | 0.08828125   |\n",
      "| explained_variance | 0.298        |\n",
      "| fps                | 22           |\n",
      "| n_updates          | 4            |\n",
      "| policy_entropy     | 1.0021622    |\n",
      "| policy_loss        | -0.010475492 |\n",
      "| serial_timesteps   | 512          |\n",
      "| time_elapsed       | 110          |\n",
      "| total_timesteps    | 2560         |\n",
      "| value_loss         | 0.22505274   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0049282135 |\n",
      "| clipfrac           | 0.036328126  |\n",
      "| explained_variance | 0.644        |\n",
      "| fps                | 25           |\n",
      "| n_updates          | 5            |\n",
      "| policy_entropy     | 0.9049994    |\n",
      "| policy_loss        | -0.009136988 |\n",
      "| serial_timesteps   | 640          |\n",
      "| time_elapsed       | 138          |\n",
      "| total_timesteps    | 3200         |\n",
      "| value_loss         | 0.57926464   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.012631627   |\n",
      "| clipfrac           | 0.15          |\n",
      "| explained_variance | 0.791         |\n",
      "| fps                | 27            |\n",
      "| n_updates          | 6             |\n",
      "| policy_entropy     | 0.87099564    |\n",
      "| policy_loss        | -0.0074533187 |\n",
      "| serial_timesteps   | 768           |\n",
      "| time_elapsed       | 164           |\n",
      "| total_timesteps    | 3840          |\n",
      "| value_loss         | 0.32028314    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.005274819  |\n",
      "| clipfrac           | 0.06015625   |\n",
      "| explained_variance | 0.515        |\n",
      "| fps                | 24           |\n",
      "| n_updates          | 7            |\n",
      "| policy_entropy     | 0.89935017   |\n",
      "| policy_loss        | -0.013674224 |\n",
      "| serial_timesteps   | 896          |\n",
      "| time_elapsed       | 187          |\n",
      "| total_timesteps    | 4480         |\n",
      "| value_loss         | 1.1936475    |\n",
      "-------------------------------------\n",
      "Saving model\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0066524083 |\n",
      "| clipfrac           | 0.058203124  |\n",
      "| explained_variance | 0.768        |\n",
      "| fps                | 24           |\n",
      "| n_updates          | 8            |\n",
      "| policy_entropy     | 0.88957214   |\n",
      "| policy_loss        | -0.012247044 |\n",
      "| serial_timesteps   | 1024         |\n",
      "| time_elapsed       | 212          |\n",
      "| total_timesteps    | 5120         |\n",
      "| value_loss         | 0.19535439   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0064507513 |\n",
      "| clipfrac           | 0.06796875   |\n",
      "| explained_variance | 0.815        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 9            |\n",
      "| policy_entropy     | 0.87822056   |\n",
      "| policy_loss        | -0.013035548 |\n",
      "| serial_timesteps   | 1152         |\n",
      "| time_elapsed       | 239          |\n",
      "| total_timesteps    | 5760         |\n",
      "| value_loss         | 0.09548789   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0058583086 |\n",
      "| clipfrac           | 0.06171875   |\n",
      "| explained_variance | 0.535        |\n",
      "| fps                | 23           |\n",
      "| n_updates          | 10           |\n",
      "| policy_entropy     | 0.86087143   |\n",
      "| policy_loss        | -0.011101693 |\n",
      "| serial_timesteps   | 1280         |\n",
      "| time_elapsed       | 263          |\n",
      "| total_timesteps    | 6400         |\n",
      "| value_loss         | 1.3780968    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0060077095  |\n",
      "| clipfrac           | 0.06796875    |\n",
      "| explained_variance | 0.562         |\n",
      "| fps                | 25            |\n",
      "| n_updates          | 11            |\n",
      "| policy_entropy     | 0.8993        |\n",
      "| policy_loss        | -0.0040680617 |\n",
      "| serial_timesteps   | 1408          |\n",
      "| time_elapsed       | 290           |\n",
      "| total_timesteps    | 7040          |\n",
      "| value_loss         | 1.6737802     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0063485196 |\n",
      "| clipfrac           | 0.0796875    |\n",
      "| explained_variance | 0.639        |\n",
      "| fps                | 27           |\n",
      "| n_updates          | 12           |\n",
      "| policy_entropy     | 0.89180535   |\n",
      "| policy_loss        | -0.008322658 |\n",
      "| serial_timesteps   | 1536         |\n",
      "| time_elapsed       | 315          |\n",
      "| total_timesteps    | 7680         |\n",
      "| value_loss         | 0.22855751   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.005229532   |\n",
      "| clipfrac           | 0.059765626   |\n",
      "| explained_variance | 0.717         |\n",
      "| fps                | 24            |\n",
      "| n_updates          | 13            |\n",
      "| policy_entropy     | 0.93577605    |\n",
      "| policy_loss        | -0.0077097514 |\n",
      "| serial_timesteps   | 1664          |\n",
      "| time_elapsed       | 339           |\n",
      "| total_timesteps    | 8320          |\n",
      "| value_loss         | 0.42295313    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0058885505  |\n",
      "| clipfrac           | 0.0734375     |\n",
      "| explained_variance | 0.411         |\n",
      "| fps                | 26            |\n",
      "| n_updates          | 14            |\n",
      "| policy_entropy     | 0.84458417    |\n",
      "| policy_loss        | -0.0053023263 |\n",
      "| serial_timesteps   | 1792          |\n",
      "| time_elapsed       | 364           |\n",
      "| total_timesteps    | 8960          |\n",
      "| value_loss         | 1.6236076     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.006565447  |\n",
      "| clipfrac           | 0.0828125    |\n",
      "| explained_variance | 0.79         |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 15           |\n",
      "| policy_entropy     | 0.8167912    |\n",
      "| policy_loss        | -0.014047506 |\n",
      "| serial_timesteps   | 1920         |\n",
      "| time_elapsed       | 388          |\n",
      "| total_timesteps    | 9600         |\n",
      "| value_loss         | 0.21412058   |\n",
      "-------------------------------------\n",
      "Saving model\n",
      "-------------------------------------\n",
      "| approxkl           | 0.008407624  |\n",
      "| clipfrac           | 0.10664062   |\n",
      "| explained_variance | 0.54         |\n",
      "| fps                | 23           |\n",
      "| n_updates          | 16           |\n",
      "| policy_entropy     | 0.8822684    |\n",
      "| policy_loss        | -0.012811393 |\n",
      "| serial_timesteps   | 2048         |\n",
      "| time_elapsed       | 413          |\n",
      "| total_timesteps    | 10240        |\n",
      "| value_loss         | 0.73066616   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0032123525 |\n",
      "| clipfrac           | 0.020703126  |\n",
      "| explained_variance | 0.566        |\n",
      "| fps                | 27           |\n",
      "| n_updates          | 17           |\n",
      "| policy_entropy     | 0.91904175   |\n",
      "| policy_loss        | -0.008036211 |\n",
      "| serial_timesteps   | 2176         |\n",
      "| time_elapsed       | 440          |\n",
      "| total_timesteps    | 10880        |\n",
      "| value_loss         | 0.8897832    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| approxkl           | 0.0046719396 |\n",
      "| clipfrac           | 0.049609374  |\n",
      "| explained_variance | 0.664        |\n",
      "| fps                | 27           |\n",
      "| n_updates          | 18           |\n",
      "| policy_entropy     | 0.8032257    |\n",
      "| policy_loss        | -0.009070677 |\n",
      "| serial_timesteps   | 2304         |\n",
      "| time_elapsed       | 463          |\n",
      "| total_timesteps    | 11520        |\n",
      "| value_loss         | 1.0958241    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0096365865 |\n",
      "| clipfrac           | 0.07578125   |\n",
      "| explained_variance | 0.697        |\n",
      "| fps                | 24           |\n",
      "| n_updates          | 19           |\n",
      "| policy_entropy     | 0.81111336   |\n",
      "| policy_loss        | -0.00876399  |\n",
      "| serial_timesteps   | 2432         |\n",
      "| time_elapsed       | 486          |\n",
      "| total_timesteps    | 12160        |\n",
      "| value_loss         | 0.19805641   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0067697666 |\n",
      "| clipfrac           | 0.0703125    |\n",
      "| explained_variance | 0.509        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 20           |\n",
      "| policy_entropy     | 0.8684365    |\n",
      "| policy_loss        | -0.009968841 |\n",
      "| serial_timesteps   | 2560         |\n",
      "| time_elapsed       | 512          |\n",
      "| total_timesteps    | 12800        |\n",
      "| value_loss         | 1.4327803    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0074356585 |\n",
      "| clipfrac           | 0.10117187   |\n",
      "| explained_variance | 0.767        |\n",
      "| fps                | 24           |\n",
      "| n_updates          | 21           |\n",
      "| policy_entropy     | 0.8553778    |\n",
      "| policy_loss        | -0.013899338 |\n",
      "| serial_timesteps   | 2688         |\n",
      "| time_elapsed       | 536          |\n",
      "| total_timesteps    | 13440        |\n",
      "| value_loss         | 0.261986     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.007799437  |\n",
      "| clipfrac           | 0.09335937   |\n",
      "| explained_variance | 0.578        |\n",
      "| fps                | 23           |\n",
      "| n_updates          | 22           |\n",
      "| policy_entropy     | 0.97567004   |\n",
      "| policy_loss        | -0.006358657 |\n",
      "| serial_timesteps   | 2816         |\n",
      "| time_elapsed       | 562          |\n",
      "| total_timesteps    | 14080        |\n",
      "| value_loss         | 0.3827322    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0033478043 |\n",
      "| clipfrac           | 0.015625     |\n",
      "| explained_variance | 0.164        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 23           |\n",
      "| policy_entropy     | 0.94634086   |\n",
      "| policy_loss        | -0.004136749 |\n",
      "| serial_timesteps   | 2944         |\n",
      "| time_elapsed       | 589          |\n",
      "| total_timesteps    | 14720        |\n",
      "| value_loss         | 1.0752681    |\n",
      "-------------------------------------\n",
      "Saving model\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0033507359 |\n",
      "| clipfrac           | 0.04765625   |\n",
      "| explained_variance | 0.507        |\n",
      "| fps                | 25           |\n",
      "| n_updates          | 24           |\n",
      "| policy_entropy     | 1.0082029    |\n",
      "| policy_loss        | -0.009068763 |\n",
      "| serial_timesteps   | 3072         |\n",
      "| time_elapsed       | 613          |\n",
      "| total_timesteps    | 15360        |\n",
      "| value_loss         | 0.5037421    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.009353282  |\n",
      "| clipfrac           | 0.11289062   |\n",
      "| explained_variance | 0.649        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 25           |\n",
      "| policy_entropy     | 0.9062687    |\n",
      "| policy_loss        | -0.007823719 |\n",
      "| serial_timesteps   | 3200         |\n",
      "| time_elapsed       | 638          |\n",
      "| total_timesteps    | 16000        |\n",
      "| value_loss         | 0.1290911    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0043037524 |\n",
      "| clipfrac           | 0.050390624  |\n",
      "| explained_variance | 0.585        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 26           |\n",
      "| policy_entropy     | 1.0357087    |\n",
      "| policy_loss        | -0.008567082 |\n",
      "| serial_timesteps   | 3328         |\n",
      "| time_elapsed       | 663          |\n",
      "| total_timesteps    | 16640        |\n",
      "| value_loss         | 0.64554167   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0066667176 |\n",
      "| clipfrac           | 0.0640625    |\n",
      "| explained_variance | 0.767        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 27           |\n",
      "| policy_entropy     | 0.9481298    |\n",
      "| policy_loss        | -0.009403613 |\n",
      "| serial_timesteps   | 3456         |\n",
      "| time_elapsed       | 687          |\n",
      "| total_timesteps    | 17280        |\n",
      "| value_loss         | 0.45884594   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0070075383 |\n",
      "| clipfrac           | 0.08164062   |\n",
      "| explained_variance | 0.857        |\n",
      "| fps                | 26           |\n",
      "| n_updates          | 28           |\n",
      "| policy_entropy     | 0.89188987   |\n",
      "| policy_loss        | -0.012261986 |\n",
      "| serial_timesteps   | 3584         |\n",
      "| time_elapsed       | 712          |\n",
      "| total_timesteps    | 17920        |\n",
      "| value_loss         | 0.2958865    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0050284495 |\n",
      "| clipfrac           | 0.0484375    |\n",
      "| explained_variance | 0.549        |\n",
      "| fps                | 24           |\n",
      "| n_updates          | 29           |\n",
      "| policy_entropy     | 0.97498065   |\n",
      "| policy_loss        | -0.005232486 |\n",
      "| serial_timesteps   | 3712         |\n",
      "| time_elapsed       | 736          |\n",
      "| total_timesteps    | 18560        |\n",
      "| value_loss         | 0.5757749    |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if LOAD_MODEL:\n",
    "    model.learn(total_timesteps=total_timesteps, tb_log_name='PPO2_75_retrain', callback=save_checkpoint_callback, reset_num_timesteps=False)\n",
    "else:\n",
    "    model.learn(total_timesteps=total_timesteps, callback=save_checkpoint_callback)\n",
    "\n",
    "## TODO:\n",
    "## add little penalty (-0.01) for using the grab action... because it grabs too much\n",
    "## DONE add little reward (0.01) for going forward... bcs it rotates too much\n",
    "## visualize attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str(log_dir) +\"/final_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "\n",
    "__Note__: Stable-Baselines requires that policy input dimensions be consistent across training and testing. Thus, the number of environments used for visualization must be a multiple of the number of environments used for training. The observation vector is then appropriately duplicated during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS_PATH = \"results/goseek-ppo-lstm-forwardreward/final_model.pkl\"\n",
    "assert MODEL_WEIGHTS_PATH, f\"Must give a model weights path!\"\n",
    "\n",
    "model = PPO2.load(str(MODEL_WEIGHTS_PATH))\n",
    "n_train_envs = model.act_model.initial_state.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize all observed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "img_shape = (-1, 240, 320, 5)\n",
    "imgs = np.reshape(obs[:, :-3], img_shape)[..., -2:]\n",
    "print(imgs.shape)\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None\n",
    "\n",
    "print(pose)\n",
    "\n",
    "assert (\n",
    "    n_train_envs % obs.shape[0] == 0\n",
    "), f\"The number of visualization environments must be a multiple of the training environments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(rgb[0])\n",
    "ax[1].imshow(segmentation[0])\n",
    "ax[2].imshow(depth[0])\n",
    "\n",
    "print(rgb[0].shape)\n",
    "print(segmentation[0].shape)\n",
    "print(depth[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an episode and plot the first person agent view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# TODO:\n",
    "# - check that the state is correct (segmentation and values of classes)\n",
    "# - check the robot pose values\n",
    "done = False\n",
    "fig, ax = plt.subplots(1, obs.shape[0])\n",
    "ax = [ax] if obs.shape[0] == 1 else ax\n",
    "\n",
    "for i in range(episode_length):\n",
    "    actions, lstm_state = model.predict(\n",
    "        np.concatenate((n_train_envs // obs.shape[0]) * [obs]),\n",
    "        state=lstm_state,\n",
    "        deterministic=False\n",
    "    )\n",
    "\n",
    "    actions = actions[: obs.shape[0]]\n",
    "\n",
    "    obs, reward, done, _ = env.step(actions)\n",
    "#     print(actions, done, reward)\n",
    "\n",
    "    plt.cla()\n",
    "    rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "    \n",
    "\n",
    "    for i in range(obs.shape[0]):\n",
    "        print(reward)\n",
    "#         print(segmentation[i][:,10])\n",
    "        print(np.max(depth[i]), np.min(depth[i]))\n",
    "        ax[i].imshow(cv2.resize(depth[i], dsize=(40,40), interpolation=cv2.INTER_NEAREST), vmin=0., vmax=1.)\n",
    "#         ax[i].imshow(depth[i], vmin=0., vmax=1.)\n",
    "\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:goseek] *",
   "language": "python",
   "name": "conda-env-goseek-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
